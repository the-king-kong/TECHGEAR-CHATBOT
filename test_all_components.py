"""
Comprehensive Test Suite for TeachGearBot Project

Tests all 14 key components:
1. Document loading
2. Chunking logic
3. Vector DB persistence
4. Retriever setup
5. Prompt template
6. Model integration
7. Chain construction
8. State definition
9. Nodes
10. Conditional routing
11. Graph compilation
12. Endpoint definition
13. Request model
14. Graph integration
15. JSON response
"""

import os
import json
from typing import TypedDict

print("\n" + "="*80)
print("üß™ COMPREHENSIVE TEACHGEARBOT TEST SUITE")
print("="*80)

# ============================================================================
# TEST 1: DOCUMENT LOADING
# ============================================================================

def test_document_loading():
    """TEST 1: Document Loading from data/product_info.txt"""
    print("\n" + "‚îÄ"*80)
    print("TEST 1: üìñ DOCUMENT LOADING")
    print("‚îÄ"*80)
    
    file_path = "data/product_info.txt"
    
    try:
        if not os.path.exists(file_path):
            print(f"‚ùå FAIL: File not found: {file_path}")
            return False
        
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        
        if len(content) > 0:
            print(f"‚úÖ PASS: Document loaded successfully")
            print(f"   File: {file_path}")
            print(f"   Size: {len(content)} characters")
            print(f"   Preview: {content[:100]}...")
            return True
        else:
            print(f"‚ùå FAIL: Document is empty")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 2: CHUNKING LOGIC
# ============================================================================

def test_chunking_logic():
    """TEST 2: Text Chunking using LangChain TextSplitter"""
    print("\n" + "‚îÄ"*80)
    print("TEST 2: ‚úÇÔ∏è  CHUNKING LOGIC")
    print("‚îÄ"*80)
    
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
        
        # Load document
        with open("data/product_info.txt", 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Create splitter
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        chunks = splitter.split_text(content)
        
        if len(chunks) > 0:
            print(f"‚úÖ PASS: Text chunking successful")
            print(f"   Total chunks: {len(chunks)}")
            print(f"   Chunk sizes: {min(len(c) for c in chunks)} - {max(len(c) for c in chunks)} chars")
            print(f"   Chunk 1: {chunks[0][:80]}...")
            return True
        else:
            print(f"‚ùå FAIL: No chunks created")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 3: VECTOR DB PERSISTENCE
# ============================================================================

def test_vector_db_persistence():
    """TEST 3: Chromadb Persistence"""
    print("\n" + "‚îÄ"*80)
    print("TEST 3: üíæ VECTOR DB PERSISTENCE")
    print("‚îÄ"*80)
    
    persist_dir = "./chroma_db"
    
    try:
        if os.path.exists(persist_dir):
            print(f"‚úÖ PASS: Chromadb directory exists")
            print(f"   Directory: {persist_dir}")
            
            contents = os.listdir(persist_dir)
            print(f"   Contents: {len(contents)} items")
            
            if len(contents) > 0:
                print(f"‚úÖ PASS: Chromadb has persisted data")
                return True
            else:
                print(f"‚ö†Ô∏è  WARNING: Chromadb directory is empty")
                return False
        else:
            print(f"‚ö†Ô∏è  WARNING: Chromadb not initialized yet (run ingest.py first)")
            return True  # Not a failure, just not initialized
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 4: RETRIEVER SETUP
# ============================================================================

def test_retriever_setup():
    """TEST 4: Retriever from Chromadb"""
    print("\n" + "‚îÄ"*80)
    print("TEST 4: üîç RETRIEVER SETUP")
    print("‚îÄ"*80)
    
    try:
        from langchain_google_genai import GoogleGenerativeAIEmbeddings
        from langchain_chroma import Chroma
        
        if not os.path.exists("./chroma_db"):
            print(f"‚ö†Ô∏è  SKIP: Chromadb not initialized")
            return True
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings,
            collection_name="product_embeddings"
        )
        
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        
        # Test retriever
        results = retriever.invoke("price")
        
        if len(results) > 0:
            print(f"‚úÖ PASS: Retriever setup successful")
            print(f"   Retrieved {len(results)} documents")
            print(f"   Sample: {results[0].page_content[:80]}...")
            return True
        else:
            print(f"‚ö†Ô∏è  WARNING: Retriever returned no results")
            return True
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 5: PROMPT TEMPLATE
# ============================================================================

def test_prompt_template():
    """TEST 5: Prompt Template Definition"""
    print("\n" + "‚îÄ"*80)
    print("TEST 5: üìù PROMPT TEMPLATE")
    print("‚îÄ"*80)
    
    try:
        from langchain_core.prompts import PromptTemplate
        
        prompt_template = """Answer ONLY using the provided context. If the answer is not in the context, say "I don't have this information."

Context:
{context}

Question: {question}

Answer:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # Test formatting
        formatted = prompt.format(
            context="SmartWatch Pro X costs ‚Çπ15,999",
            question="What is the price?"
        )
        
        if "SmartWatch" in formatted and "‚Çπ15,999" in formatted:
            print(f"‚úÖ PASS: Prompt template created and formatted correctly")
            print(f"   Input variables: {prompt.input_variables}")
            print(f"   Template preview: {prompt_template[:100]}...")
            return True
        else:
            print(f"‚ùå FAIL: Prompt formatting failed")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 6: MODEL INTEGRATION
# ============================================================================

def test_model_integration():
    """TEST 6: Gemini LLM Integration"""
    print("\n" + "‚îÄ"*80)
    print("TEST 6: ü§ñ MODEL INTEGRATION")
    print("‚îÄ"*80)
    
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print(f"‚ö†Ô∏è  SKIP: GOOGLE_API_KEY not set")
            return True
        
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            temperature=0
        )
        
        # Test LLM
        response = llm.invoke("Say 'Model Integration Test Successful'")
        
        if response and hasattr(response, 'content'):
            print(f"‚úÖ PASS: LLM model integrated successfully")
            print(f"   Model: gemini-2.0-flash")
            print(f"   Response: {response.content}")
            return True
        else:
            print(f"‚ùå FAIL: LLM response invalid")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 7: CHAIN CONSTRUCTION
# ============================================================================

def test_chain_construction():
    """TEST 7: RAG Chain Construction"""
    print("\n" + "‚îÄ"*80)
    print("TEST 7: ‚õìÔ∏è  CHAIN CONSTRUCTION")
    print("‚îÄ"*80)
    
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
        from langchain_chroma import Chroma
        from langchain_core.prompts import PromptTemplate
        from langchain_core.runnables import RunnablePassthrough
        from langchain_core.output_parsers import StrOutputParser
        
        if not os.path.exists("./chroma_db"):
            print(f"‚ö†Ô∏è  SKIP: Chromadb not initialized")
            return True
        
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings,
            collection_name="product_embeddings"
        )
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        
        prompt = PromptTemplate(
            template="Answer: {context}\n\nQ: {question}\n\nA:",
            input_variables=["context", "question"]
        )
        
        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)
        
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        print(f"‚úÖ PASS: Chain constructed successfully")
        print(f"   Chain type: RunnableSequence (LCEL)")
        print(f"   Components: Retriever ‚Üí Prompt ‚Üí LLM ‚Üí Parser")
        return True
        
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 8: STATE DEFINITION
# ============================================================================

def test_state_definition():
    """TEST 8: LangGraph State Definition"""
    print("\n" + "‚îÄ"*80)
    print("TEST 8: üìã STATE DEFINITION")
    print("‚îÄ"*80)
    
    try:
        class GraphState(TypedDict):
            query: str
            category: str
            response: str
            escalation_reason: str
        
        # Test state instantiation
        test_state = {
            "query": "What is the price?",
            "category": "product",
            "response": "",
            "escalation_reason": ""
        }
        
        print(f"‚úÖ PASS: GraphState defined with TypedDict")
        print(f"   Fields: query, category, response, escalation_reason")
        print(f"   Sample state: {test_state}")
        return True
        
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 9: NODES
# ============================================================================

def test_nodes():
    """TEST 9: LangGraph Nodes"""
    print("\n" + "‚îÄ"*80)
    print("TEST 9: üîß NODES")
    print("‚îÄ"*80)
    
    try:
        # Test classifier node logic
        def classifier_node(state):
            query = state["query"].lower()
            if "price" in query:
                return {"category": "product"}
            elif "return" in query:
                return {"category": "returns"}
            else:
                return {"category": "general"}
        
        # Test node with sample data
        test_input = {"query": "What is the price?", "category": "", "response": "", "escalation_reason": ""}
        result = classifier_node(test_input)
        
        if result["category"] == "product":
            print(f"‚úÖ PASS: Nodes defined and working")
            print(f"   Classifier node: Working")
            print(f"   Test: 'What is the price?' ‚Üí category: '{result['category']}'")
            return True
        else:
            print(f"‚ùå FAIL: Node logic incorrect")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 10: CONDITIONAL ROUTING
# ============================================================================

def test_conditional_routing():
    """TEST 10: Conditional Routing"""
    print("\n" + "‚îÄ"*80)
    print("TEST 10: üîÄ CONDITIONAL ROUTING")
    print("‚îÄ"*80)
    
    try:
        def should_escalate(state):
            category = state.get("category", "")
            if category in ["product", "returns"]:
                return "rag_responder"
            else:
                return "escalation"
        
        # Test cases
        test_cases = [
            ({"category": "product"}, "rag_responder"),
            ({"category": "returns"}, "rag_responder"),
            ({"category": "general"}, "escalation")
        ]
        
        all_pass = True
        for state, expected in test_cases:
            result = should_escalate(state)
            if result != expected:
                all_pass = False
                print(f"‚ùå Test failed: {state} ‚Üí {result} (expected {expected})")
        
        if all_pass:
            print(f"‚úÖ PASS: Conditional routing working correctly")
            print(f"   product/returns ‚Üí rag_responder")
            print(f"   general ‚Üí escalation")
            return True
        else:
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 11: GRAPH COMPILATION
# ============================================================================

def test_graph_compilation():
    """TEST 11: Graph Compilation"""
    print("\n" + "‚îÄ"*80)
    print("TEST 11: ‚õìÔ∏è  GRAPH COMPILATION")
    print("‚îÄ"*80)
    
    try:
        from langgraph.graph import StateGraph, START, END
        from typing import TypedDict
        
        class TestState(TypedDict):
            query: str
            category: str
        
        def node1(state):
            return {"category": "product"}
        
        def node2(state):
            return {}
        
        workflow = StateGraph(TestState)
        workflow.add_node("classifier", node1)
        workflow.add_node("responder", node2)
        workflow.add_edge(START, "classifier")
        workflow.add_edge("classifier", "responder")
        workflow.add_edge("responder", END)
        
        graph = workflow.compile()
        
        print(f"‚úÖ PASS: Graph compiled successfully")
        print(f"   Graph type: CompiledGraph")
        print(f"   Nodes: classifier, responder")
        print(f"   Edges: START‚Üíclassifier‚Üíresponder‚ÜíEND")
        return True
        
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 12: ENDPOINT DEFINITION
# ============================================================================

def test_endpoint_definition():
    """TEST 12: FastAPI Endpoint Definition"""
    print("\n" + "‚îÄ"*80)
    print("TEST 12: üåê ENDPOINT DEFINITION")
    print("‚îÄ"*80)
    
    try:
        from fastapi import FastAPI
        from pydantic import BaseModel
        
        app = FastAPI()
        
        class TestRequest(BaseModel):
            query: str
        
        @app.post("/chat")
        async def chat(request: TestRequest):
            return {"response": "test"}
        
        print(f"‚úÖ PASS: Endpoint defined successfully")
        print(f"   Method: POST")
        print(f"   Route: /chat")
        print(f"   Handler: async chat()")
        return True
        
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 13: REQUEST MODEL
# ============================================================================

def test_request_model():
    """TEST 13: Pydantic Request Model"""
    print("\n" + "‚îÄ"*80)
    print("TEST 13: üì® REQUEST MODEL")
    print("‚îÄ"*80)
    
    try:
        from pydantic import BaseModel
        
        class ChatRequest(BaseModel):
            query: str
        
        # Test model validation
        valid_data = ChatRequest(query="What is the price?")
        
        if valid_data.query == "What is the price?":
            print(f"‚úÖ PASS: Request model working correctly")
            print(f"   Model: ChatRequest")
            print(f"   Fields: query (str)")
            print(f"   Sample: {valid_data}")
            return True
        else:
            print(f"‚ùå FAIL: Request model validation failed")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 14: GRAPH INTEGRATION
# ============================================================================

def test_graph_integration():
    """TEST 14: Graph Integration with API"""
    print("\n" + "‚îÄ"*80)
    print("TEST 14: üîó GRAPH INTEGRATION")
    print("‚îÄ"*80)
    
    try:
        # Check if graph.py exists and has process_query
        from graph import process_query
        
        print(f"‚úÖ PASS: Graph module imported successfully")
        print(f"   Module: graph.py")
        print(f"   Function: process_query()")
        print(f"   Status: Ready for integration with API")
        return True
        
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# TEST 15: JSON RESPONSE
# ============================================================================

def test_json_response():
    """TEST 15: JSON Response Format"""
    print("\n" + "‚îÄ"*80)
    print("TEST 15: üì§ JSON RESPONSE")
    print("‚îÄ"*80)
    
    try:
        from pydantic import BaseModel
        
        class ChatResponse(BaseModel):
            query: str
            response: str
        
        # Test response generation
        response_data = ChatResponse(
            query="What is the price?",
            response="‚Çπ15,999"
        )
        
        # Convert to JSON
        json_str = response_data.model_dump_json()
        parsed = json.loads(json_str)
        
        if "query" in parsed and "response" in parsed:
            print(f"‚úÖ PASS: JSON response format valid")
            print(f"   Model: ChatResponse")
            print(f"   Fields: query, response")
            print(f"   Sample JSON: {json_str}")
            return True
        else:
            print(f"‚ùå FAIL: JSON response format invalid")
            return False
            
    except Exception as e:
        print(f"‚ùå FAIL: {e}")
        return False


# ============================================================================
# RUN ALL TESTS
# ============================================================================

def main():
    """Run all 15 tests"""
    
    tests = [
        ("Document Loading", test_document_loading),
        ("Chunking Logic", test_chunking_logic),
        ("Vector DB Persistence", test_vector_db_persistence),
        ("Retriever Setup", test_retriever_setup),
        ("Prompt Template", test_prompt_template),
        ("Model Integration", test_model_integration),
        ("Chain Construction", test_chain_construction),
        ("State Definition", test_state_definition),
        ("Nodes", test_nodes),
        ("Conditional Routing", test_conditional_routing),
        ("Graph Compilation", test_graph_compilation),
        ("Endpoint Definition", test_endpoint_definition),
        ("Request Model", test_request_model),
        ("Graph Integration", test_graph_integration),
        ("JSON Response", test_json_response),
    ]
    
    results = []
    
    for i, (name, test_func) in enumerate(tests, 1):
        result = test_func()
        results.append((name, result))
    
    # Summary
    print("\n" + "="*80)
    print("üìä TEST SUMMARY")
    print("="*80)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for i, (name, result) in enumerate(results, 1):
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{i:2d}. {name:<30} {status}")
    
    print("="*80)
    print(f"Total: {passed}/{total} tests passed ({int(passed/total*100)}%)")
    print("="*80 + "\n")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! Your TeachGearBot is fully functional!")
    else:
        print(f"‚ö†Ô∏è  {total - passed} test(s) failed. Review the output above for details.")


if __name__ == "__main__":
    main()
